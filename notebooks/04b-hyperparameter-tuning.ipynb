{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4b: Hyperparameter Tuning with Optuna\n",
    "\n",
    "This notebook uses Optuna to find optimal hyperparameters for our advanced models.\n",
    "\n",
    "**Objectives:**\n",
    "- Use Optuna for automated hyperparameter optimization\n",
    "- Tune LightGBM, XGBoost, and CatBoost independently\n",
    "- Use time-series CV for robust evaluation\n",
    "- Compare tuned results to manually specified hyperparameters\n",
    "- Save best hyperparameters for final model training\n",
    "\n",
    "**Strategy:**\n",
    "- Each model gets its own Optuna study\n",
    "- Use pruning to stop unpromising trials early\n",
    "- Target metric: RMSPE (minimize)\n",
    "- Budget: 50-100 trials per model (adjustable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import json\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "from evaluation.cv import (\n",
    "    make_time_series_folds,\n",
    "    filter_open_stores,\n",
    "    remove_missing_features\n",
    ")\n",
    "from evaluation.metrics import rmspe\n",
    "from models.train_baselines import get_feature_columns\n",
    "from utils.io import read_parquet, ensure_dir\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Optuna logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Ensure output directories\n",
    "ensure_dir('../outputs/tuning')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Create CV Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load featured data\n",
    "df = read_parquet('../data/processed/train_features.parquet')\n",
    "\n",
    "print(f\"Loaded data shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CV config\n",
    "with open('../config/params.yaml', 'r') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "cv_config = params['cv']\n",
    "\n",
    "print(\"Cross-Validation Configuration:\")\n",
    "print(f\"  N folds: {cv_config['n_folds']}\")\n",
    "print(f\"  Fold length: {cv_config['fold_length_days']} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and create folds\n",
    "df_open = filter_open_stores(df)\n",
    "\n",
    "# Create folds\n",
    "folds = make_time_series_folds(\n",
    "    df_open,\n",
    "    n_folds=cv_config['n_folds'],\n",
    "    fold_length_days=cv_config['fold_length_days'],\n",
    "    min_train_days=cv_config['min_train_days']\n",
    ")\n",
    "\n",
    "feature_cols = get_feature_columns(df_open)\n",
    "\n",
    "print(f\"\\nCreated {len(folds)} folds\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Hyperparameter Search Spaces\n",
    "\n",
    "For each model, we define the hyperparameters to tune and their search ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trials per model\n",
    "N_TRIALS = 50  # Increase to 100+ for better results (but takes longer)\n",
    "\n",
    "print(f\"Hyperparameter tuning configuration:\")\n",
    "print(f\"  Trials per model: {N_TRIALS}\")\n",
    "print(f\"  CV folds: {len(folds)}\")\n",
    "print(f\"  Estimated time per model: {N_TRIALS * len(folds) * 10 / 60:.0f} minutes (rough estimate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tune LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lightgbm(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for LightGBM hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Evaluate using CV\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "        # Get train and validation data\n",
    "        train_data = df_open.iloc[train_idx].copy()\n",
    "        val_data = df_open.iloc[val_idx].copy()\n",
    "        \n",
    "        # Remove rows with missing features\n",
    "        train_data, valid_features = remove_missing_features(train_data, feature_cols)\n",
    "        val_data, _ = remove_missing_features(val_data, valid_features)\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train = train_data[valid_features]\n",
    "        y_train = train_data['Sales']\n",
    "        X_val = val_data[valid_features]\n",
    "        y_val = val_data['Sales']\n",
    "        \n",
    "        # Create LightGBM datasets\n",
    "        train_set = lgb.Dataset(X_train, label=y_train)\n",
    "        val_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_set,\n",
    "            num_boost_round=2000,\n",
    "            valid_sets=[val_set],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                lgb.log_evaluation(period=0)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        score = rmspe(y_val.values, y_pred)\n",
    "        fold_scores.append(score)\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(np.mean(fold_scores), fold_idx)\n",
    "        \n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run LightGBM study\n",
    "print(\"=\"*60)\n",
    "print(\"Tuning LightGBM Hyperparameters\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study_lgb = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name='lightgbm_tuning',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    ")\n",
    "\n",
    "study_lgb.optimize(objective_lightgbm, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest RMSPE: {study_lgb.best_value:.6f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgboost(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'seed': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # Evaluate using CV\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "        # Get train and validation data\n",
    "        train_data = df_open.iloc[train_idx].copy()\n",
    "        val_data = df_open.iloc[val_idx].copy()\n",
    "        \n",
    "        # Remove rows with missing features\n",
    "        train_data, valid_features = remove_missing_features(train_data, feature_cols)\n",
    "        val_data, _ = remove_missing_features(val_data, valid_features)\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train = train_data[valid_features].copy()\n",
    "        y_train = train_data['Sales']\n",
    "        X_val = val_data[valid_features].copy()\n",
    "        y_val = val_data['Sales']\n",
    "        \n",
    "        # Convert categoricals to codes for XGBoost\n",
    "        for col in X_train.columns:\n",
    "            if X_train[col].dtype.name == 'category':\n",
    "                X_train[col] = X_train[col].cat.codes\n",
    "                X_val[col] = X_val[col].cat.codes\n",
    "        \n",
    "        # Create XGBoost datasets\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        # Train model\n",
    "        evals = [(dval, 'valid')]\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=2000,\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(dval, iteration_range=(0, model.best_iteration + 1))\n",
    "        score = rmspe(y_val.values, y_pred)\n",
    "        fold_scores.append(score)\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(np.mean(fold_scores), fold_idx)\n",
    "        \n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run XGBoost study\n",
    "print(\"=\"*60)\n",
    "print(\"Tuning XGBoost Hyperparameters\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study_xgb = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name='xgboost_tuning',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    ")\n",
    "\n",
    "study_xgb.optimize(objective_xgboost, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest RMSPE: {study_xgb.best_value:.6f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tune CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_catboost(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for CatBoost hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'depth': trial.suggest_int('depth', 5, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.0, 1.0),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'verbose': False,\n",
    "        'random_seed': 42\n",
    "    }\n",
    "    \n",
    "    # Identify categorical features\n",
    "    cat_features = [\n",
    "        col for col in feature_cols\n",
    "        if df_open[col].dtype in ['object', 'category']\n",
    "    ]\n",
    "    \n",
    "    # Evaluate using CV\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "        # Get train and validation data\n",
    "        train_data = df_open.iloc[train_idx].copy()\n",
    "        val_data = df_open.iloc[val_idx].copy()\n",
    "        \n",
    "        # Remove rows with missing features\n",
    "        train_data, valid_features = remove_missing_features(train_data, feature_cols)\n",
    "        val_data, _ = remove_missing_features(val_data, valid_features)\n",
    "        \n",
    "        # Update categorical features\n",
    "        valid_cat_features = [col for col in cat_features if col in valid_features]\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train = train_data[valid_features]\n",
    "        y_train = train_data['Sales']\n",
    "        X_val = val_data[valid_features]\n",
    "        y_val = val_data['Sales']\n",
    "        \n",
    "        # Create CatBoost datasets\n",
    "        train_pool = cb.Pool(X_train, label=y_train, cat_features=valid_cat_features)\n",
    "        val_pool = cb.Pool(X_val, label=y_val, cat_features=valid_cat_features)\n",
    "        \n",
    "        # Train model\n",
    "        model = cb.CatBoost(params)\n",
    "        model.fit(\n",
    "            train_pool,\n",
    "            eval_set=val_pool,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(val_pool)\n",
    "        score = rmspe(y_val.values, y_pred)\n",
    "        fold_scores.append(score)\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(np.mean(fold_scores), fold_idx)\n",
    "        \n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run CatBoost study\n",
    "print(\"=\"*60)\n",
    "print(\"Tuning CatBoost Hyperparameters\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study_cb = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name='catboost_tuning',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    ")\n",
    "\n",
    "study_cb.optimize(objective_catboost, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest RMSPE: {study_cb.best_value:.6f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for key, value in study_cb.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manual hyperparameter results\n",
    "with open(\"../outputs/metrics/advanced/lightgbm_tuned_cv_results.json\", \"r\") as f:\n",
    "    manual_lgb = json.load(f)\n",
    "\n",
    "with open(\"../outputs/metrics/advanced/xgboost_cv_results.json\", \"r\") as f:\n",
    "    manual_xgb = json.load(f)\n",
    "\n",
    "with open(\"../outputs/metrics/advanced/catboost_cv_results.json\", \"r\") as f:\n",
    "    manual_cb = json.load(f)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'XGBoost', 'CatBoost'],\n",
    "    'Manual RMSPE': [\n",
    "        float(manual_lgb['mean_score']),\n",
    "        float(manual_xgb['mean_score']),\n",
    "        float(manual_cb['mean_score'])\n",
    "    ],\n",
    "    'Optuna RMSPE': [\n",
    "        study_lgb.best_value,\n",
    "        study_xgb.best_value,\n",
    "        study_cb.best_value\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison['Improvement (%)'] = (\n",
    "    (comparison['Manual RMSPE'] - comparison['Optuna RMSPE']) / \n",
    "    comparison['Manual RMSPE'] * 100\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Hyperparameter Tuning Results\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nComparison: Manual vs Optuna-tuned\")\n",
    "display(comparison)\n",
    "\n",
    "print(f\"\\nBest overall model: {comparison.loc[comparison['Optuna RMSPE'].idxmin(), 'Model']}\")\n",
    "print(f\"Best RMSPE: {comparison['Optuna RMSPE'].min():.6f}\")\n",
    "\n",
    "# Gap to target\n",
    "target = 0.09856\n",
    "best_score = comparison['Optuna RMSPE'].min()\n",
    "gap = best_score - target\n",
    "gap_pct = (gap / target) * 100\n",
    "\n",
    "print(f\"\\nGap to target (0.09856): {gap:.6f} ({gap_pct:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# LightGBM\n",
    "ax = axes[0]\n",
    "trials_lgb = study_lgb.trials\n",
    "ax.plot([t.number for t in trials_lgb], [t.value for t in trials_lgb], 'o-', alpha=0.5)\n",
    "ax.axhline(y=study_lgb.best_value, color='r', linestyle='--', label=f'Best: {study_lgb.best_value:.6f}')\n",
    "ax.axhline(y=target, color='g', linestyle='--', label=f'Target: {target:.6f}')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('RMSPE')\n",
    "ax.set_title('LightGBM Optimization History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# XGBoost\n",
    "ax = axes[1]\n",
    "trials_xgb = study_xgb.trials\n",
    "ax.plot([t.number for t in trials_xgb], [t.value for t in trials_xgb], 'o-', alpha=0.5)\n",
    "ax.axhline(y=study_xgb.best_value, color='r', linestyle='--', label=f'Best: {study_xgb.best_value:.6f}')\n",
    "ax.axhline(y=target, color='g', linestyle='--', label=f'Target: {target:.6f}')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('RMSPE')\n",
    "ax.set_title('XGBoost Optimization History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# CatBoost\n",
    "ax = axes[2]\n",
    "trials_cb = study_cb.trials\n",
    "ax.plot([t.number for t in trials_cb], [t.value for t in trials_cb], 'o-', alpha=0.5)\n",
    "ax.axhline(y=study_cb.best_value, color='r', linestyle='--', label=f'Best: {study_cb.best_value:.6f}')\n",
    "ax.axhline(y=target, color='g', linestyle='--', label=f'Target: {target:.6f}')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('RMSPE')\n",
    "ax.set_title('CatBoost Optimization History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/tuning/optimization_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: outputs/tuning/optimization_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, comparison['Manual RMSPE'], width, label='Manual', alpha=0.7)\n",
    "ax.bar(x + width/2, comparison['Optuna RMSPE'], width, label='Optuna', alpha=0.7)\n",
    "\n",
    "ax.axhline(y=target, color='red', linestyle='--', label='Target (0.09856)', linewidth=2)\n",
    "ax.set_ylabel('RMSPE')\n",
    "ax.set_title('Manual vs Optuna-Tuned Hyperparameters')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/tuning/manual_vs_optuna.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: outputs/tuning/manual_vs_optuna.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best hyperparameters for each model\n",
    "best_params = {\n",
    "    'lightgbm': study_lgb.best_params,\n",
    "    'xgboost': study_xgb.best_params,\n",
    "    'catboost': study_cb.best_params,\n",
    "    'scores': {\n",
    "        'lightgbm': study_lgb.best_value,\n",
    "        'xgboost': study_xgb.best_value,\n",
    "        'catboost': study_cb.best_value\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../outputs/tuning/best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "\n",
    "print(\"Saved best hyperparameters to: outputs/tuning/best_hyperparameters.json\")\n",
    "print(\"\\nThese can be used in config/params.yaml for final model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Hyperparameter Tuning Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOptimized {len(comparison)} models with {N_TRIALS} trials each\")\n",
    "print(f\"\\nResults:\")\n",
    "for idx, row in comparison.iterrows():\n",
    "    print(f\"  {row['Model']}:\")\n",
    "    print(f\"    Manual:  {row['Manual RMSPE']:.6f}\")\n",
    "    print(f\"    Optuna:  {row['Optuna RMSPE']:.6f}\")\n",
    "    print(f\"    Improvement: {row['Improvement (%)']:.2f}%\")\n",
    "\n",
    "print(f\"\\nBest model: {comparison.loc[comparison['Optuna RMSPE'].idxmin(), 'Model']}\")\n",
    "print(f\"Best RMSPE: {best_score:.6f}\")\n",
    "print(f\"Gap to target: {gap_pct:+.2f}%\")\n",
    "\n",
    "print(f\"\\nBest hyperparameters saved to: outputs/tuning/best_hyperparameters.json\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
