{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 4b: Hyperparameter Tuning with Optuna\n",
    "\n",
    "This notebook uses Optuna to find optimal hyperparameters for our advanced models.\n",
    "\n",
    "**Objectives:**\n",
    "- Use Optuna for automated hyperparameter optimization\n",
    "- Tune LightGBM, XGBoost, and CatBoost independently\n",
    "- Use time-series CV for robust evaluation\n",
    "- Compare tuned results to manually specified hyperparameters\n",
    "- Track all experiments in MLflow\n",
    "- Save best hyperparameters for final model training\n",
    "\n",
    "**Strategy:**\n",
    "- Each model gets its own Optuna study\n",
    "- Use pruning to stop unpromising trials early\n",
    "- Target metric: RMSPE (minimize)\n",
    "- Budget: 50-100 trials per model (adjustable)\n",
    "\n",
    "**CRITICAL:** All experiments tracked with MLflow for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import json\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import mlflow\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "\n",
    "from evaluation.cv import (\n",
    "    make_time_series_folds,\n",
    "    filter_open_stores,\n",
    "    remove_missing_features\n",
    ")\n",
    "from evaluation.metrics import rmspe\n",
    "from utils.io import read_parquet\n",
    "from utils.mlflow_utils import setup_mlflow, log_dvc_data_version\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Optuna logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Setup MLflow\n",
    "experiment_id = setup_mlflow()\n",
    "print(f\"MLflow experiment ID: {experiment_id}\")\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Data and Create CV Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load featured data\n",
    "df = read_parquet('../data/processed/train_features.parquet')\n",
    "\n",
    "print(f\"Loaded data shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CV config\n",
    "with open('../config/params.yaml', 'r') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "cv_config = params['cv']\n",
    "\n",
    "print(\"Cross-Validation Configuration:\")\n",
    "print(f\"  N folds: {cv_config['n_folds']}\")\n",
    "print(f\"  Fold length: {cv_config['fold_length_days']} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and create folds\n",
    "df_open = filter_open_stores(df)\n",
    "\n",
    "# Create folds\n",
    "folds = make_time_series_folds(\n",
    "    df_open,\n",
    "    n_folds=cv_config['n_folds'],\n",
    "    fold_length_days=cv_config['fold_length_days'],\n",
    "    min_train_days=cv_config['min_train_days']\n",
    ")\n",
    "\n",
    "# Define feature columns\n",
    "exclude_cols = ['Sales', 'Date', 'Store', 'Customers']\n",
    "feature_cols = [col for col in df_open.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nCreated {len(folds)} folds\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Define Hyperparameter Search Spaces\n",
    "\n",
    "For each model, we define the hyperparameters to tune and their search ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trials per model\n",
    "N_TRIALS = 50  # Increase to 100+ for better results (but takes longer)\n",
    "\n",
    "print(f\"Hyperparameter tuning configuration:\")\n",
    "print(f\"  Trials per model: {N_TRIALS}\")\n",
    "print(f\"  CV folds: {len(folds)}\")\n",
    "print(f\"  Estimated time per model: {N_TRIALS * len(folds) * 10 / 60:.0f} minutes (rough estimate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Tune LightGBM\n",
    "\n",
    "Use Optuna to find optimal LightGBM hyperparameters.\n",
    "\n",
    "**Tracked in MLflow:** Each trial logged as nested run with parameters and RMSPE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lightgbm(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for LightGBM hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Evaluate using CV\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "        # Get train and validation data\n",
    "        train_data = df_open.iloc[train_idx].copy()\n",
    "        val_data = df_open.iloc[val_idx].copy()\n",
    "        \n",
    "        # Remove rows with missing features\n",
    "        train_data, valid_features = remove_missing_features(train_data, feature_cols)\n",
    "        val_data, _ = remove_missing_features(val_data, valid_features)\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train = train_data[valid_features]\n",
    "        y_train = train_data['Sales']\n",
    "        X_val = val_data[valid_features]\n",
    "        y_val = val_data['Sales']\n",
    "        \n",
    "        # Create LightGBM datasets\n",
    "        train_set = lgb.Dataset(X_train, label=y_train)\n",
    "        val_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_set,\n",
    "            num_boost_round=2000,\n",
    "            valid_sets=[val_set],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                lgb.log_evaluation(period=0)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        score = rmspe(y_val.values, y_pred)\n",
    "        fold_scores.append(score)\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(np.mean(fold_scores), fold_idx)\n",
    "        \n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start parent MLflow run for LightGBM tuning\n",
    "with mlflow.start_run(run_name=\"lightgbm_optuna_tuning\") as parent_run:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Tuning LightGBM Hyperparameters\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Log DVC data version\n",
    "    log_dvc_data_version(\"data/processed/train_features.parquet\")\n",
    "    \n",
    "    # Log tuning metadata\n",
    "    mlflow.log_param(\"model_type\", \"lightgbm\")\n",
    "    mlflow.log_param(\"tuning_method\", \"optuna\")\n",
    "    mlflow.log_param(\"n_trials\", N_TRIALS)\n",
    "    mlflow.log_param(\"n_folds\", len(folds))\n",
    "    mlflow.log_param(\"pruner\", \"MedianPruner\")\n",
    "    \n",
    "    # Create MLflow callback for Optuna\n",
    "    mlflc = MLflowCallback(\n",
    "        tracking_uri=mlflow.get_tracking_uri(),\n",
    "        metric_name=\"rmspe\",\n",
    "        create_experiment=False,\n",
    "        mlflow_kwargs={\"nested\": True}\n",
    "    )\n",
    "    \n",
    "    # Create and run study\n",
    "    study_lgb = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name='lightgbm_tuning',\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "    )\n",
    "    \n",
    "    study_lgb.optimize(\n",
    "        objective_lightgbm,\n",
    "        n_trials=N_TRIALS,\n",
    "        callbacks=[mlflc],\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Log best results\n",
    "    mlflow.log_metric(\"best_rmspe\", study_lgb.best_value)\n",
    "    mlflow.log_metric(\"n_trials_completed\", len(study_lgb.trials))\n",
    "    \n",
    "    # Log best hyperparameters\n",
    "    for key, value in study_lgb.best_params.items():\n",
    "        mlflow.log_param(f\"best_{key}\", value)\n",
    "    \n",
    "    print(f\"\\nBest RMSPE: {study_lgb.best_value:.6f}\")\n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    for key, value in study_lgb.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nMLflow Run ID: {parent_run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Tune XGBoost\n",
    "\n",
    "Use Optuna to find optimal XGBoost hyperparameters.\n",
    "\n",
    "**Tracked in MLflow:** Each trial logged as nested run with parameters and RMSPE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgboost(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'seed': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # Evaluate using CV\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "        # Get train and validation data\n",
    "        train_data = df_open.iloc[train_idx].copy()\n",
    "        val_data = df_open.iloc[val_idx].copy()\n",
    "        \n",
    "        # Remove rows with missing features\n",
    "        train_data, valid_features = remove_missing_features(train_data, feature_cols)\n",
    "        val_data, _ = remove_missing_features(val_data, valid_features)\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train = train_data[valid_features].copy()\n",
    "        y_train = train_data['Sales']\n",
    "        X_val = val_data[valid_features].copy()\n",
    "        y_val = val_data['Sales']\n",
    "        \n",
    "        # Convert categoricals to codes for XGBoost\n",
    "        for col in X_train.columns:\n",
    "            if X_train[col].dtype.name == 'category':\n",
    "                X_train[col] = X_train[col].cat.codes\n",
    "                X_val[col] = X_val[col].cat.codes\n",
    "        \n",
    "        # Create XGBoost datasets\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        # Train model\n",
    "        evals = [(dval, 'valid')]\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=2000,\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(dval, iteration_range=(0, model.best_iteration + 1))\n",
    "        score = rmspe(y_val.values, y_pred)\n",
    "        fold_scores.append(score)\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(np.mean(fold_scores), fold_idx)\n",
    "        \n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start parent MLflow run for XGBoost tuning\n",
    "with mlflow.start_run(run_name=\"xgboost_optuna_tuning\") as parent_run:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Tuning XGBoost Hyperparameters\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Log DVC data version\n",
    "    log_dvc_data_version(\"data/processed/train_features.parquet\")\n",
    "    \n",
    "    # Log tuning metadata\n",
    "    mlflow.log_param(\"model_type\", \"xgboost\")\n",
    "    mlflow.log_param(\"tuning_method\", \"optuna\")\n",
    "    mlflow.log_param(\"n_trials\", N_TRIALS)\n",
    "    mlflow.log_param(\"n_folds\", len(folds))\n",
    "    mlflow.log_param(\"pruner\", \"MedianPruner\")\n",
    "    \n",
    "    # Create MLflow callback for Optuna\n",
    "    mlflc = MLflowCallback(\n",
    "        tracking_uri=mlflow.get_tracking_uri(),\n",
    "        metric_name=\"rmspe\",\n",
    "        create_experiment=False,\n",
    "        mlflow_kwargs={\"nested\": True}\n",
    "    )\n",
    "    \n",
    "    # Create and run study\n",
    "    study_xgb = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name='xgboost_tuning',\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "    )\n",
    "    \n",
    "    study_xgb.optimize(\n",
    "        objective_xgboost,\n",
    "        n_trials=N_TRIALS,\n",
    "        callbacks=[mlflc],\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Log best results\n",
    "    mlflow.log_metric(\"best_rmspe\", study_xgb.best_value)\n",
    "    mlflow.log_metric(\"n_trials_completed\", len(study_xgb.trials))\n",
    "    \n",
    "    # Log best hyperparameters\n",
    "    for key, value in study_xgb.best_params.items():\n",
    "        mlflow.log_param(f\"best_{key}\", value)\n",
    "    \n",
    "    print(f\"\\nBest RMSPE: {study_xgb.best_value:.6f}\")\n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    for key, value in study_xgb.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nMLflow Run ID: {parent_run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 5. Tune CatBoost\n",
    "\n",
    "Use Optuna to find optimal CatBoost hyperparameters.\n",
    "\n",
    "**Tracked in MLflow:** Each trial logged as nested run with parameters and RMSPE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_catboost(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for CatBoost hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'depth': trial.suggest_int('depth', 5, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.0, 1.0),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'verbose': False,\n",
    "        'random_seed': 42\n",
    "    }\n",
    "    \n",
    "    # Identify categorical features\n",
    "    cat_features = [\n",
    "        col for col in feature_cols\n",
    "        if df_open[col].dtype in ['object', 'category']\n",
    "    ]\n",
    "    \n",
    "    # Evaluate using CV\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "        # Get train and validation data\n",
    "        train_data = df_open.iloc[train_idx].copy()\n",
    "        val_data = df_open.iloc[val_idx].copy()\n",
    "        \n",
    "        # Remove rows with missing features\n",
    "        train_data, valid_features = remove_missing_features(train_data, feature_cols)\n",
    "        val_data, _ = remove_missing_features(val_data, valid_features)\n",
    "        \n",
    "        # Update categorical features\n",
    "        valid_cat_features = [col for col in cat_features if col in valid_features]\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train = train_data[valid_features]\n",
    "        y_train = train_data['Sales']\n",
    "        X_val = val_data[valid_features]\n",
    "        y_val = val_data['Sales']\n",
    "        \n",
    "        # Create CatBoost datasets\n",
    "        train_pool = cb.Pool(X_train, label=y_train, cat_features=valid_cat_features)\n",
    "        val_pool = cb.Pool(X_val, label=y_val, cat_features=valid_cat_features)\n",
    "        \n",
    "        # Train model\n",
    "        model = cb.CatBoost(params)\n",
    "        model.fit(\n",
    "            train_pool,\n",
    "            eval_set=val_pool,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(val_pool)\n",
    "        score = rmspe(y_val.values, y_pred)\n",
    "        fold_scores.append(score)\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(np.mean(fold_scores), fold_idx)\n",
    "        \n",
    "        # Prune unpromising trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return np.mean(fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start parent MLflow run for CatBoost tuning\n",
    "with mlflow.start_run(run_name=\"catboost_optuna_tuning\") as parent_run:\n",
    "    print(\"=\"*60)\n",
    "    print(\"Tuning CatBoost Hyperparameters\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Log DVC data version\n",
    "    log_dvc_data_version(\"data/processed/train_features.parquet\")\n",
    "    \n",
    "    # Log tuning metadata\n",
    "    mlflow.log_param(\"model_type\", \"catboost\")\n",
    "    mlflow.log_param(\"tuning_method\", \"optuna\")\n",
    "    mlflow.log_param(\"n_trials\", N_TRIALS)\n",
    "    mlflow.log_param(\"n_folds\", len(folds))\n",
    "    mlflow.log_param(\"pruner\", \"MedianPruner\")\n",
    "    \n",
    "    # Create MLflow callback for Optuna\n",
    "    mlflc = MLflowCallback(\n",
    "        tracking_uri=mlflow.get_tracking_uri(),\n",
    "        metric_name=\"rmspe\",\n",
    "        create_experiment=False,\n",
    "        mlflow_kwargs={\"nested\": True}\n",
    "    )\n",
    "    \n",
    "    # Create and run study\n",
    "    study_cb = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name='catboost_tuning',\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
    "    )\n",
    "    \n",
    "    study_cb.optimize(\n",
    "        objective_catboost,\n",
    "        n_trials=N_TRIALS,\n",
    "        callbacks=[mlflc],\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Log best results\n",
    "    mlflow.log_metric(\"best_rmspe\", study_cb.best_value)\n",
    "    mlflow.log_metric(\"n_trials_completed\", len(study_cb.trials))\n",
    "    \n",
    "    # Log best hyperparameters\n",
    "    for key, value in study_cb.best_params.items():\n",
    "        mlflow.log_param(f\"best_{key}\", value)\n",
    "    \n",
    "    print(f\"\\nBest RMSPE: {study_cb.best_value:.6f}\")\n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    for key, value in study_cb.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nMLflow Run ID: {parent_run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get manual hyperparameter scores from MLflow experiments\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name(\"rossmann-forecasting\")\n",
    "\n",
    "# Find runs by name\n",
    "manual_lgb_runs = client.search_runs(\n",
    "    experiment.experiment_id,\n",
    "    filter_string=\"tags.mlflow.runName = 'lightgbm_tuned'\",\n",
    "    max_results=1\n",
    ")\n",
    "manual_xgb_runs = client.search_runs(\n",
    "    experiment.experiment_id,\n",
    "    filter_string=\"tags.mlflow.runName = 'xgboost'\",\n",
    "    max_results=1\n",
    ")\n",
    "manual_cb_runs = client.search_runs(\n",
    "    experiment.experiment_id,\n",
    "    filter_string=\"tags.mlflow.runName = 'catboost'\",\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "# Extract scores\n",
    "manual_lgb_score = manual_lgb_runs[0].data.metrics['rmspe_mean'] if manual_lgb_runs else 0.136\n",
    "manual_xgb_score = manual_xgb_runs[0].data.metrics['rmspe_mean'] if manual_xgb_runs else 0.130\n",
    "manual_cb_score = manual_cb_runs[0].data.metrics['rmspe_mean'] if manual_cb_runs else 0.136\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'XGBoost', 'CatBoost'],\n",
    "    'Manual RMSPE': [\n",
    "        manual_lgb_score,\n",
    "        manual_xgb_score,\n",
    "        manual_cb_score\n",
    "    ],\n",
    "    'Optuna RMSPE': [\n",
    "        study_lgb.best_value,\n",
    "        study_xgb.best_value,\n",
    "        study_cb.best_value\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison['Improvement (%)'] = (\n",
    "    (comparison['Manual RMSPE'] - comparison['Optuna RMSPE']) / \n",
    "    comparison['Manual RMSPE'] * 100\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Hyperparameter Tuning Results\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nComparison: Manual vs Optuna-tuned\")\n",
    "display(comparison)\n",
    "\n",
    "print(f\"\\nBest overall model: {comparison.loc[comparison['Optuna RMSPE'].idxmin(), 'Model']}\")\n",
    "print(f\"Best RMSPE: {comparison['Optuna RMSPE'].min():.6f}\")\n",
    "\n",
    "# Gap to target\n",
    "target = 0.09856\n",
    "best_score = comparison['Optuna RMSPE'].min()\n",
    "gap = best_score - target\n",
    "gap_pct = (gap / target) * 100\n",
    "\n",
    "print(f\"\\nGap to target (0.09856): {gap:.6f} ({gap_pct:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. Visualize Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# LightGBM\n",
    "ax = axes[0]\n",
    "trials_lgb = study_lgb.trials\n",
    "ax.plot([t.number for t in trials_lgb], [t.value for t in trials_lgb], 'o-', alpha=0.5)\n",
    "ax.axhline(y=study_lgb.best_value, color='r', linestyle='--', label=f'Best: {study_lgb.best_value:.6f}')\n",
    "ax.axhline(y=target, color='g', linestyle='--', label=f'Target: {target:.6f}')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('RMSPE')\n",
    "ax.set_title('LightGBM Optimization History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# XGBoost\n",
    "ax = axes[1]\n",
    "trials_xgb = study_xgb.trials\n",
    "ax.plot([t.number for t in trials_xgb], [t.value for t in trials_xgb], 'o-', alpha=0.5)\n",
    "ax.axhline(y=study_xgb.best_value, color='r', linestyle='--', label=f'Best: {study_xgb.best_value:.6f}')\n",
    "ax.axhline(y=target, color='g', linestyle='--', label=f'Target: {target:.6f}')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('RMSPE')\n",
    "ax.set_title('XGBoost Optimization History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# CatBoost\n",
    "ax = axes[2]\n",
    "trials_cb = study_cb.trials\n",
    "ax.plot([t.number for t in trials_cb], [t.value for t in trials_cb], 'o-', alpha=0.5)\n",
    "ax.axhline(y=study_cb.best_value, color='r', linestyle='--', label=f'Best: {study_cb.best_value:.6f}')\n",
    "ax.axhline(y=target, color='g', linestyle='--', label=f'Target: {target:.6f}')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('RMSPE')\n",
    "ax.set_title('CatBoost Optimization History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, comparison['Manual RMSPE'], width, label='Manual', alpha=0.7)\n",
    "ax.bar(x + width/2, comparison['Optuna RMSPE'], width, label='Optuna', alpha=0.7)\n",
    "\n",
    "ax.axhline(y=target, color='red', linestyle='--', label='Target (0.09856)', linewidth=2)\n",
    "ax.set_ylabel('RMSPE')\n",
    "ax.set_title('Manual vs Optuna-Tuned Hyperparameters')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Hyperparameter Tuning Complete!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. Hyperparameter Optimization:\")\n",
    "print(f\"   Optimized {len(comparison)} models with {N_TRIALS} trials each\")\n",
    "print(f\"\\n   Individual Model Results:\")\n",
    "for idx, row in comparison.iterrows():\n",
    "    print(f\"     {row['Model']}:\")\n",
    "    print(f\"       Manual:  {row['Manual RMSPE']:.6f}\")\n",
    "    print(f\"       Optuna:  {row['Optuna RMSPE']:.6f}\")\n",
    "    print(f\"       Improvement: {row['Improvement (%)']:.2f}%\")\n",
    "\n",
    "print(f\"\\n2. Best Overall:\")\n",
    "best_model = comparison.loc[comparison['Optuna RMSPE'].idxmin(), 'Model']\n",
    "best_score = comparison['Optuna RMSPE'].min()\n",
    "print(f\"   Model: {best_model}\")\n",
    "print(f\"   RMSPE: {best_score:.6f}\")\n",
    "print(f\"   Gap to target (0.09856): {gap:.6f} ({gap_pct:+.2f}%)\")\n",
    "\n",
    "print(f\"\\n3. MLflow Tracking:\")\n",
    "print(f\"   All trials tracked as nested runs in MLflow\")\n",
    "print(f\"   View results: bash scripts/start_mlflow.sh\")\n",
    "print(f\"   Then open: http://127.0.0.1:5000\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
